version: '3.8'

services:

  almacenamiento:
    build: ./almacenamiento
    ports:
      - "8000:8000"
    environment:
      - MONGO_URI=mongodb+srv://${MONGO_USER}:${MONGO_PASSWORD}@${MONGO_CLUSTER}/${MONGO_DB}?retryWrites=true&w=majority&tls=true&authSource=admin
      - MONGO_USER=${MONGO_USER}
      - MONGO_PASSWORD=${MONGO_PASSWORD}
      - MONGO_CLUSTER=${MONGO_CLUSTER}
      - MONGO_DB=${MONGO_DB}
      - MONGO_COLLECTION=${MONGO_COLLECTION}

  scraper:
    build:
      context: ./scraper
    environment:
      - URL_ALMACENAMIENTO=http://almacenamiento:8000/eventos
    depends_on:
      - almacenamiento

  filtrador:
    build: ./filtrador
    depends_on:
      - almacenamiento
    env_file:
      - .env
    command: python filtrador.py
    volumes:
      - ./filtrador/outputs:/app/outputs

  procesador:
    image: suhothayan/hadoop-spark-pig-hive:2.9.2
    container_name: pigprocessor
    user: root
    volumes:
      - ./filtrador/outputs:/root/dataset
      - ./procesador:/root/scripts
      - ./resultados_pig:/root/results
    command: sh -c "
      mkdir -p /usr/local/spark/conf/ &&
      touch /usr/local/spark/conf/spark-env.sh &&
      chmod -R 777 /usr/local/spark/conf/ &&
      service ssh start &&
      sleep 10 &&
      /usr/local/hadoop/bin/hdfs dfsadmin -safemode wait &&
      echo 'Sistema listo. Ejecuta manualmente:' &&
      echo '1. hdfs dfs -put /root/dataset/eventos_filtrados.csv /user/hadoop/' &&
      echo '2. pig -x mapreduce -f /root/scripts/trafico_analisis.pig' &&
      tail -f /dev/null"
    ports: 
      - "50070:50070"
      - "8088:8088"
      - "8080:8080"
